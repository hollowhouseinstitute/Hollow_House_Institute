---
author: "Hollow House Institute"
last_updated: "2025-12-02"
status: "DRAFT â€” Needs legal review"
---

# AI Safety Policy

## Mission Statement

The Hollow House Institute is committed to advancing AI safety through trauma-informed, ethical research practices. This policy outlines our commitment to preventing harmful AI applications and promoting beneficial AI development.

## Core Principles

### 1. Trauma-Informed AI Development
All AI systems developed using our datasets must:
- Recognize and respond to trauma indicators
- Avoid re-traumatization
- Prioritize user safety and consent
- Implement appropriate content warnings

### 2. Human Dignity and Autonomy
AI systems must:
- Respect human agency and decision-making
- Avoid manipulative or coercive patterns
- Provide transparent operation
- Enable user control and opt-out mechanisms

### 3. Harm Prevention
Researchers and developers must actively work to prevent:
- Psychological manipulation
- Surveillance and privacy violations
- Discriminatory or biased outcomes
- Exploitation of vulnerable populations

## Prohibited Uses

The following uses of Hollow House Institute datasets are **strictly prohibited**:

### Absolutely Prohibited
- **Weapons development**: Any military, weapons, or surveillance systems
- **Manipulation**: Systems designed to manipulate, deceive, or exploit users
- **Discrimination**: Systems that discriminate based on protected characteristics
- **Unauthorized surveillance**: Monitoring individuals without explicit informed consent
- **Harmful profiling**: Creating profiles for punitive or discriminatory purposes
- **Re-identification**: Attempting to identify anonymized research participants

### Requires Special Approval
- **Commercial therapeutic tools**: Must undergo additional ethical review
- **Crisis intervention systems**: Requires demonstration of safety protocols
- **Child-focused applications**: Enhanced privacy and safety requirements
- **Healthcare decision support**: Must meet regulatory and ethical standards

## Required Safeguards

All AI systems developed using our datasets must implement:

### Technical Safeguards
1. **Bias Testing**: Regular evaluation for discriminatory patterns
2. **Safety Limits**: Rate limiting and usage boundaries
3. **Monitoring**: Ongoing monitoring for misuse or harm
4. **Fail-safes**: Graceful degradation and human oversight options

### Operational Safeguards
1. **Human Oversight**: Meaningful human review of high-stakes decisions
2. **Transparency**: Clear communication about AI involvement
3. **Accountability**: Designated responsible parties
4. **Incident Response**: Plans for addressing harm or misuse

### Ethical Safeguards
1. **Ethics Review**: Independent ethical review for deployment
2. **User Consent**: Clear informed consent procedures
3. **Right to Appeal**: Human review of AI-influenced decisions
4. **Data Minimization**: Collect and retain only necessary data

## Red Team Testing

Before deployment, AI systems must undergo:
- Adversarial testing for misuse scenarios
- Evaluation by diverse user groups
- Trauma-informed review by mental health professionals
- Third-party security assessment

## Incident Reporting

All safety incidents must be reported to safety@hollowhouseinstitute.org within 24 hours:
- User harm or distress
- Privacy breaches
- Discriminatory outcomes
- System failures in crisis situations
- Unauthorized access or use

## Enforcement

Violations of this policy may result in:
1. Immediate suspension of data access
2. Required corrective actions
3. Permanent revocation of license
4. Public disclosure of violations (where appropriate)
5. Legal action for serious violations

## Model Deployment Requirements

Before deploying AI models trained on our data:

1. **Documentation**: Complete [Model Card Template](./model_card_template.md)
2. **Testing**: Conduct comprehensive safety evaluation
3. **Review**: Submit to our ethical review board (Enterprise tier)
4. **Monitoring Plan**: Establish ongoing safety monitoring
5. **Update Protocol**: Plan for addressing discovered issues

## Co-Regulation Framework

AI systems should be designed according to our Relational AI Co-Regulation Framework (see diagram in assets/diagrams/), which emphasizes:
- Bidirectional communication
- Adaptive responses
- Emotional attunement
- Ethical boundaries
- User empowerment

## Research Ethics

All research using our datasets must:
- Undergo institutional ethics review (IRB or equivalent)
- Follow established research ethics guidelines
- Publish findings responsibly
- Share safety-relevant findings with the community
- Acknowledge dataset use and limitations

## Continuous Improvement

We commit to:
- Regular policy updates based on emerging risks
- Community input on safety practices
- Collaboration with AI safety researchers
- Transparent reporting of known issues
- Supporting safety-focused research

## Resources

Additional resources for AI safety:
- [Ethics and Limits](./ethics_and_limits.md)
- [Anonymization Protocol](./anonymization_protocol.md)
- [Model Card Template](./model_card_template.md)
- Partnership for AI: https://partnershiponai.org
- AI Incident Database: https://incidentdatabase.ai

## Contact

For AI safety questions or to report concerns:
- Safety incidents: safety@hollowhouseinstitute.org
- Policy questions: policy@hollowhouseinstitute.org
- General ethics: ethics@hollowhouseinstitute.org

---

**Note**: This is a draft document and requires legal review before implementation. We welcome community feedback on strengthening these safety provisions.
